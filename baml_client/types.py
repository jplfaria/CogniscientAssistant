###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off
import baml_py
from enum import Enum

from pydantic import BaseModel, ConfigDict

from typing_extensions import TypeAlias, Literal
from typing import Dict, Generic, List, Optional, TypeVar, Union


T = TypeVar('T')
CheckName = TypeVar('CheckName', bound=str)

class Check(BaseModel):
    name: str
    expression: str
    status: str
class Checked(BaseModel, Generic[T,CheckName]):
    value: T
    checks: Dict[CheckName, Check]

def get_checks(checks: Dict[CheckName, Check]) -> List[Check]:
    return list(checks.values())

def all_succeeded(checks: Dict[CheckName, Check]) -> bool:
    return all(check.status == "succeeded" for check in get_checks(checks))



class AgentType(str, Enum):
    
    Generation = "Generation"
    Reflection = "Reflection"
    Ranking = "Ranking"
    Evolution = "Evolution"
    Proximity = "Proximity"
    MetaReview = "MetaReview"
    Supervisor = "Supervisor"

class ConfidenceLevel(str, Enum):
    
    High = "High"
    Medium = "Medium"
    Low = "Low"

class Criticality(str, Enum):
    
    Fundamental = "Fundamental"
    Peripheral = "Peripheral"

class HypothesisCategory(str, Enum):
    
    Mechanistic = "Mechanistic"
    Therapeutic = "Therapeutic"
    Diagnostic = "Diagnostic"
    Biomarker = "Biomarker"
    Methodology = "Methodology"
    Other = "Other"

class RequestType(str, Enum):
    
    Generate = "Generate"
    Analyze = "Analyze"
    Evaluate = "Evaluate"
    Compare = "Compare"

class ResponseStatus(str, Enum):
    
    Success = "Success"
    Error = "Error"
    Partial = "Partial"

class ReviewDecision(str, Enum):
    
    Accept = "Accept"
    Reject = "Reject"
    Revise = "Revise"

class ReviewType(str, Enum):
    
    Initial = "Initial"
    Full = "Full"
    DeepVerification = "DeepVerification"
    Observation = "Observation"
    Simulation = "Simulation"
    Tournament = "Tournament"

class SafetyCategory(str, Enum):
    
    ContentSafety = "ContentSafety"
    GoalViolation = "GoalViolation"
    EthicalConcern = "EthicalConcern"
    ResourceMisuse = "ResourceMisuse"

class SafetyLevel(str, Enum):
    
    Safe = "Safe"
    Concerning = "Concerning"
    Blocked = "Blocked"

class TaskState(str, Enum):
    
    Pending = "Pending"
    Assigned = "Assigned"
    Executing = "Executing"
    Completed = "Completed"
    Failed = "Failed"

class TaskType(str, Enum):
    
    GenerateHypothesis = "GenerateHypothesis"
    ReflectOnHypothesis = "ReflectOnHypothesis"
    RankHypotheses = "RankHypotheses"
    EvolveHypothesis = "EvolveHypothesis"
    FindSimilarHypotheses = "FindSimilarHypotheses"
    MetaReview = "MetaReview"

class Validity(str, Enum):
    
    Valid = "Valid"
    Questionable = "Questionable"
    Invalid = "Invalid"

class AgentRequest(BaseModel):
    request_id: str
    agent_type: "AgentType"
    request_type: "RequestType"
    content: "RequestContent"

class AgentResponse(BaseModel):
    request_id: str
    status: "ResponseStatus"
    response: Optional["ResponseData"] = None
    error: Optional["ErrorInfo"] = None

class AssumptionDecomposition(BaseModel):
    assumption: str
    validity: str
    evidence: str
    criticality: str

class Citation(BaseModel):
    authors: List[str]
    title: str
    journal: Optional[str] = None
    year: int
    doi: Optional[str] = None
    url: Optional[str] = None

class ComparisonResult(BaseModel):
    winner_id: str
    confidence: float
    reasoning: str
    strengths_h1: List[str]
    strengths_h2: List[str]
    decisive_factors: List[str]

class ErrorInfo(BaseModel):
    code: str
    message: str
    recoverable: bool

class ExperimentalProtocol(BaseModel):
    objective: str
    methodology: str
    required_resources: List[str]
    timeline: str
    success_metrics: List[str]
    potential_challenges: List[str]
    safety_considerations: List[str]

class FailurePoint(BaseModel):
    step: str
    probability: float
    impact: str

class Hypothesis(BaseModel):
    id: str
    summary: str
    category: str
    full_description: str
    novelty_claim: str
    assumptions: List[str]
    experimental_protocol: "ExperimentalProtocol"
    supporting_evidence: List["Citation"]
    confidence_score: float
    generation_method: str
    created_at: str
    elo_rating: Optional[float] = None
    review_count: Optional[int] = None
    evolution_count: Optional[int] = None

class HypothesisSummary(BaseModel):
    core_idea: str
    scientific_impact: str
    feasibility_assessment: str
    next_steps: List[str]

class ParsedResearchGoal(BaseModel):
    primary_objective: str
    sub_objectives: List[str]
    implied_constraints: List[str]
    suggested_categories: List[str]
    key_terms: List[str]
    success_criteria: List[str]

class RequestContent(BaseModel):
    prompt: str
    context: Dict[str, str]
    parameters: Dict[str, str]

class ResearchPatterns(BaseModel):
    identified_patterns: List[str]
    common_strengths: List[str]
    common_weaknesses: List[str]
    emerging_themes: List[str]
    recommendations: List[str]
    synthesis_summary: str

class ResponseData(BaseModel):
    content: str
    metadata: Dict[str, str]

class Review(BaseModel):
    id: str
    hypothesis_id: str
    reviewer_agent_id: str
    review_type: "ReviewType"
    decision: "ReviewDecision"
    scores: "ReviewScores"
    narrative_feedback: str
    key_strengths: List[str]
    key_weaknesses: List[str]
    improvement_suggestions: List[str]
    confidence_level: str
    assumption_decomposition: Optional[List["AssumptionDecomposition"]] = None
    simulation_results: Optional["SimulationResults"] = None
    literature_citations: Optional[List["Citation"]] = None
    created_at: str
    time_spent_seconds: Optional[float] = None

class ReviewScores(BaseModel):
    correctness: float
    quality: float
    novelty: float
    safety: float
    feasibility: float

class SafetyCheck(BaseModel):
    id: str
    target_type: str
    target_id: str
    safety_level: "SafetyLevel"
    passed: bool
    checks_performed: List[str]
    violations: Optional[List[str]] = None
    recommendations: Optional[List[str]] = None
    category: Optional["SafetyCategory"] = None
    timestamp: str
    metadata: Optional[Dict[str, str]] = None

class SimilarityScore(BaseModel):
    overall_similarity: float
    aspect_scores: Dict[str, float]
    shared_concepts: List[str]
    key_differences: List[str]

class SimulationResults(BaseModel):
    mechanism_steps: List[str]
    failure_points: List["FailurePoint"]
    predicted_outcomes: List[str]

class Task(BaseModel):
    id: str
    task_type: "TaskType"
    priority: int
    state: "TaskState"
    payload: Dict[str, str]
    assigned_to: Optional[str] = None
    result: Optional[Dict[str, str]] = None
    error: Optional[str] = None
    created_at: str
    assigned_at: Optional[str] = None
    completed_at: Optional[str] = None
