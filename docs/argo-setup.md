# Argo Gateway Setup Guide

This document describes how to set up and use the Argo gateway for LLM access in the AI Co-Scientist system.

## Overview

The Argo gateway provides a unified interface to multiple LLM providers (OpenAI, Anthropic, Google) through a local proxy server. This allows the system to:

- Use multiple models through a single endpoint
- Handle authentication and rate limiting centrally
- Switch between providers seamlessly
- Monitor usage and costs

## Prerequisites

1. Python 3.11+ installed
2. Valid API credentials for LLM providers
3. Network access to Argo endpoints (may require VPN)

## Installation

1. **Install the argo-proxy package:**
   ```bash
   pip install argo-proxy
   ```

2. **Set up environment variables:**
   - Copy `.env.example` to `.env`
   - Set your `ARGO_USER` username in the `.env` file
   - Configure other settings as needed

## Configuration

The system uses two configuration files:

### 1. Environment Variables (.env)

```bash
# Required
ARGO_USER=your_username_here

# Optional (with defaults)
ARGO_PROXY_HOST=127.0.0.1
ARGO_PROXY_PORT=8000
ARGO_PROXY_URL=http://localhost:8000/v1
```

### 2. Argo Configuration (argo-config.yaml)

This file is automatically generated by the startup script with your username and settings.

## Starting the Proxy

Use the provided startup script:

```bash
./scripts/start-argo-proxy.sh
```

This script will:
1. Check for required environment variables
2. Generate the configuration file
3. Start the proxy server
4. Display the access URL

## Testing Connectivity

After starting the proxy, test connectivity using:

```bash
./scripts/test-argo-proxy.py
```

This will:
1. Check if the proxy is running
2. List available models
3. Test chat completion with each model
4. Provide a summary of results

## Available Models

The following models are configured:

- **gpt4o** - OpenAI GPT-4 Optimized
- **gpt-3.5-turbo** - OpenAI GPT-3.5 Turbo
- **claude-opus-4** - Anthropic Claude Opus 4
- **gemini-2.5-pro** - Google Gemini 2.5 Pro

## Integration with BAML

The Argo gateway integrates with BAML through the LLM abstraction layer. Model mappings are configured in:

- `baml_src/clients.baml` - BAML client configurations
- `src/llm/argo_provider.py` - Argo provider implementation (to be created)

## Troubleshooting

### Proxy won't start
- Check if `.env` file exists and contains `ARGO_USER`
- Ensure no other process is using port 8000
- Check logs for specific error messages

### Models not accessible
- Verify VPN connection if required
- Check API credentials are valid
- Ensure model names match exactly
- Review rate limits and quotas

### Connection timeouts
- Increase timeout values in configuration
- Check network connectivity
- Verify firewall settings

## Security Notes

- **Never commit** your username or API keys
- Keep `.env` in `.gitignore`
- Use environment variables for all sensitive data
- Monitor usage to prevent unauthorized access

## Next Steps

After setting up the Argo gateway:

1. Create the ArgoLLMProvider implementation
2. Configure BAML clients to use Argo
3. Test end-to-end LLM calls
4. Implement model routing logic